{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "qifqfg25mv",
   "metadata": {},
   "source": [
    "# Optimus Prime - MD5 Hash Inversion Training\n",
    "\n",
    "```\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣶⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⠀⠀⠀⠀⠀⣤⣤⣤⠀⠀⠀⠀⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡇⠀⣠⡶⢿⡇⢿⣿⡏⢳⣦⠀⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⡛⣆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣧⣼⣿⣴⣋⡽⠮⠿⢭⣟⣏⣷⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⣧⠘⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡼⣇⣿⡿⠶⣶⣿⣟⡛⣷⣿⢠⠙⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⡈⣏⠇⢹⡀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡟⢹⠁⣿⠋⠉⢹⠉⠙⣿⡇⣾⣀⣾⠀⢀⣤⡀⢀⡀⠀⠀⢀⣠⣴⣾⠛⢻⡛⢻⡄⢀⣳⡀⢀⣠⠄⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⣷⣾⢀⣿⡇⠀⠸⠀⠀⣿⣧⡽⠿⣟⣺⣭⠴⢿⡏⣩⣷⡾⢛⣭⣴⣿⣇⠘⣿⣷⣿⡛⠉⢻⣟⣷⠄⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠿⢿⣟⣿⣿⡦⣶⣪⡭⠿⣚⣫⣭⣽⣶⡄⠀⢸⡇⣿⡙⣿⣿⣿⣿⣿⣿⣆⠹⣿⣿⣷⡀⠀⢿⡉⠁⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⣤⣶⣿⠿⠛⣉⣭⣶⣾⣿⠿⠟⠛⠉⠉⢻⠀⢸⣷⣿⣇⢻⡿⣿⣿⣿⣿⠟⠀⠹⣿⣿⠃⠀⠘⣷⡀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣤⣦⣼⣿⠿⠛⣋⡁⣼⢠⣿⡿⠛⠉⠁⠀⠀⢀⡀⢀⣴⣾⠀⢸⣿⡇⢻⡄⠙⠿⠻⠛⠁⠀⢀⣠⣽⣿⣇⡀⠀⠸⣧⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⠿⣛⣭⣴⡾⠟⠛⣧⣿⢸⡿⠀⠀⠀⠀⣰⣿⣿⣷⣾⣿⣿⠀⢸⡏⣇⢸⣷⡀⠀⢀⣠⣴⣾⠿⠛⣿⢻⣿⣹⡀⠀⢻⣆⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣴⡟⣦⠀⠀⠀⢀⡿⣵⡿⠛⠉⣡⣶⣤⣄⣿⣯⢸⣇⠀⠀⢠⣾⣿⡿⣿⣿⣿⣿⡿⠀⢸⡇⢻⡼⣿⣷⣶⠿⠛⠉⠀⠀⠀⠸⡇⣿⣿⣧⠀⠘⣿⡀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⢹⠀⢀⣠⣼⣿⣿⠀⢀⣼⣿⣿⣿⣿⡇⣿⢸⣿⣀⣀⣿⡿⠿⠶⠚⠛⠉⠉⠀⠀⢸⡇⠀⢻⣾⣝⣿⡆⠀⢀⣠⡴⠖⠛⢻⡾⣿⣿⣆⠀⢹⡇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣇⣼⡾⠟⠋⣿⢻⣇⣤⣌⠻⢿⣿⣿⣿⠃⢿⠀⠉⠉⠁⠀⠀⠀⣀⣤⡤⠶⠶⠒⠚⣻⣷⣄⠈⣿⣿⣿⣿⡞⠉⠀⠀⠀⠀⠀⣿⢿⣿⣾⣋⣽⠇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣹⠏⠀⠀⠀⣿⢿⣿⣿⣯⡴⠾⠛⢋⣡⠶⠛⠛⠋⣉⣉⣉⣙⢻⣿⠀⠀⠀⠀⠀⢠⡟⠀⠈⠻⢦⣈⣿⣿⣧⠀⠀⢀⣠⣴⡾⢿⣿⣿⣿⣿⣿⡀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡟⣿⡟⠀⠀⠀⣿⠈⠋⠉⢀⣠⠴⣛⣩⣤⣶⣞⣭⣿⢿⣿⣿⣻⣼⣿⣆⣀⣤⣤⣴⣿⣄⣠⣶⣦⣀⣙⣿⣿⣿⡶⣿⠟⠋⣁⣶⠟⢻⣽⣿⣿⣿⠇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⢠⣿⣇⠀⠀⠀⢹⣠⡴⠖⢻⣷⢫⣿⣿⣿⣯⣿⣟⣿⣿⣭⣽⣿⡿⣿⣿⣿⠿⠿⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⣿⠋⠉⣿⠀⢸⣿⣿⣿⣿⣷⡀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣼⣿⣿⣤⣴⣾⢿⡅⠀⣀⣾⢿⣿⣿⣿⣿⣿⣿⡿⣿⣷⣿⣿⣿⡇⣿⣿⡇⠀⠀⢸⣿⣿⡟⢿⣿⣿⣿⣿⣿⣣⣿⠁⣿⣀⣤⡿⠀⢀⣿⣿⣿⣿⣿⡇⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡇⠻⣿⠛⠉⠀⠈⣿⠛⢽⣿⢻⣿⣿⢿⣿⣿⣿⡇⣿⠿⣶⣶⣚⣧⣿⣿⡇⠀⠀⣸⣿⣿⣿⣄⣈⢿⣿⢿⣷⣿⣿⠀⠉⠉⠀⠀⠀⠘⡇⣿⣿⣿⣿⡇⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡇⡀⣷⡆⠀⠀⠀⠸⣧⣻⣿⢸⣿⣿⡿⢿⣾⣻⡇⣿⣿⣿⣿⣿⣿⣿⠿⠷⠾⠛⠛⠿⢿⣿⣿⣿⣄⣿⠿⠋⢸⣿⠀⠀⠀⠀⠀⠀⠀⡇⣿⣿⣿⣿⣿⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣷⡇⣿⡇⠀⠀⠀⠀⣿⣿⣿⡾⢿⣿⣿⣿⣿⡶⠷⠾⠛⠛⠉⠁⢀⣠⠤⠴⠒⡆⢠⠀⢰⡉⠻⣿⣽⡏⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⠀⡇⣿⡿⣿⣿⣿⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣧⣿⠿⢀⣀⣤⣴⣿⣿⣿⡷⠾⠛⠋⠉⢀⣀⣠⠤⠴⠒⠻⡆⢸⠀⠀⢀⡠⠇⠸⡄⠈⣇⠀⠈⡻⢦⡀⠀⢸⡇⠀⠀⠀⠀⠀⠀⠀⡇⣿⣧⡘⠿⢻⡆\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠻⣆⣿⣿⣿⣿⣿⡿⠛⣉⣀⡀⣠⠴⠒⠋⠉⠁⠀⠀⠀⠀⠀⡇⢸⣠⠴⣫⡄⠀⠀⡇⠀⢹⠀⠀⣿⠦⢿⡀⢸⡇⠀⠀⣀⣤⣤⣿⠀⡇⣿⣿⣿⣆⢸⡇\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣿⢿⡟⣽⣿⠀⣏⠁⠀⡇⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣇⠀⡖⣻⠋⠀⠀⠈⢻⠀⢈⡇⠀⠸⡄⠘⣧⢸⡇⠀⢸⣷⣾⣿⠏⠀⡇⣿⣿⣿⣿⢸⡇\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣾⠏⠛⠋⢡⣿⠀⠸⣿⣟⡃⣇⠀⠀⠀⠀⠀⣀⣠⡤⠶⠒⠋⠀⠛⠁⠀⣀⣤⣶⣿⣿⣿⣿⣷⣤⡈⠁⢻⡞⣿⠀⠈⠻⣴⠏⠀⠀⠿⢹⣿⣎⢻⣿⡇\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣾⡟⠀⠀⢀⡿⣿⠀⠀⠈⠳⡇⠻⠤⠶⠚⠋⠉⠁⠀⠀⠀⠀⠀⣀⣤⣶⣿⣿⣿⣿⣿⠿⠛⠻⣿⣿⣿⣷⣜⣷⣿⠀⠀⢀⣀⣤⣤⣶⣾⣶⣿⣿⠃⢸⡇\n",
    "⠀⠀⠀⠀⠀⠀⣀⣤⡶⠶⠖⠚⢛⠛⠳⢶⣼⡟⠀⠀⢀⣼⣹⣿⢀⠀⠀⠀⠀⡀⠀⠀⠀⠀⠀⢀⣀⣠⡤⢤⣾⣿⣿⣿⡿⠿⠛⠉⠹⡇⠀⠀⣿⣿⣟⢿⣿⣿⠹⣶⣿⡿⠛⠻⣏⠀⠉⠉⡛⣿⡿⣾⡇\n",
    "⠀⠀⠀⢀⣴⠞⠋⢰⡇⢰⣿⢻⢻⢻⢶⣦⠙⣷⡀⠀⣸⢧⠟⢿⣿⣿⣿⣷⣶⣶⣤⣴⣲⡾⠿⠟⠒⠒⠛⡇⠙⣿⠉⠀⢧⠀⠀⠀⠀⣧⠀⠀⢸⣿⣿⡎⣿⠁⢀⣼⣏⢀⣠⣤⣸⣶⠀⠀⣿⣿⣿⠛⠁\n",
    "⠀⠀⠀⣾⠃⠀⣠⡬⣤⣼⣛⠾⣼⣞⡾⡟⠀⠘⣧⣠⣏⡞⠀⠈⠻⣿⡏⢹⡟⠛⠻⣿⠁⠀⠀⠀⠀⠀⠀⣇⠀⣿⠀⠀⢸⡄⠀⠀⠀⢸⠀⠀⠘⣿⣿⣇⣿⣴⡞⢣⣽⣿⣿⣿⣿⣿⠀⠀⣿⣿⡟⠀⠀\n",
    "⠀⠀⠀⣿⡶⣿⣿⣸⣿⣿⣿⠿⠷⠾⢽⣅⡲⠶⢻⣿⣼⢁⣠⣤⣶⣿⣿⠘⡇⠀⠀⢻⡆⠀⠀⠀⠀⠀⢀⣸⡀⢹⡇⠀⠈⡇⠀⠀⠀⠈⡇⠀⠀⢿⣿⣿⢹⣿⣤⣿⣿⣿⣿⡿⢿⣟⡀⠀⣿⣿⡇⠀⠀\n",
    "⠀⠀⠀⠈⠛⠿⢯⣜⣿⠏⠀⠀⠀⢀⡿⣨⣿⣶⣤⣿⣷⣯⣿⣿⣿⣿⣿⠀⡇⠀⠀⠐⡿⣦⣰⣒⣶⣿⣿⣿⣷⣾⣇⠀⠀⢻⠀⠀⠀⠀⢷⠀⠀⢸⣿⣿⣾⣿⣸⣿⡏⢠⠟⣠⣿⣿⣿⣦⡈⢹⡇⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⢸⡟⣾⠄⠀⠀⣸⡇⣿⣿⣿⠟⠋⠛⢿⣿⣿⣿⣿⣿⡄⢻⠀⠀⠀⡇⠈⠙⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⢸⡆⠀⠀⠀⢸⡄⠀⠀⣿⣿⣇⣿⠛⠛⠻⣿⣺⣿⣿⣿⣿⣿⣿⡿⠃⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⣼⢧⡇⠀⠀⠀⣿⢸⣿⣿⡿⢦⣴⣿⣿⣷⡿⣿⡿⣿⡇⢸⡄⠀⠀⢹⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⡆⠀⠀⣇⠀⠀⠀⠀⣇⠀⠀⢸⣿⣟⢿⡀⠀⠀⠈⠉⠀⠉⠉⠉⠁⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⣿⣨⡧⠤⠤⢤⣇⡾⣿⣿⣠⣿⣿⣿⣿⣿⣿⣽⣿⣿⣷⠀⣇⠀⠀⢸⠀⠀⢸⢻⣿⣿⣿⣿⡇⣿⣿⠀⠀⢹⡄⠀⠀⢀⣸⠀⠀⠸⣿⣿⣼⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⢀⡿⣧⣤⠶⠦⣼⣿⣿⣿⡏⠈⣿⣿⢿⣿⣿⣿⣏⠉⢹⣿⡀⢻⠀⠀⠘⡇⠀⠸⡄⠙⢿⣿⣿⠇⣿⣿⡄⠀⠈⠓⠒⠋⠉⠀⠀⠀⠀⢿⠹⣯⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⣸⣿⢃⡏⠀⠀⢻⣿⣿⣽⣿⣦⠘⣿⣿⣿⣿⣿⢻⣿⣾⣿⡇⠘⡇⠀⠀⣇⠀⠀⣇⠀⠀⠙⢿⡇⣿⢸⣧⠀⠀⠀⠀⡴⠒⢶⠀⠀⠀⠘⣆⠀⢻⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⡿⡅⣸⢁⣄⡄⣾⣿⢿⣿⠿⣿⣿⢻⣿⣿⣟⣿⣸⣻⡿⣿⣧⠀⠙⠒⠛⠛⠀⠀⢿⣿⣄⠀⠀⠀⣿⠈⣿⡄⠀⠀⠀⡇⠀⠘⡇⠀⠀⠀⢿⣦⢸⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⢸⣧⡇⣿⣼⣿⠃⣿⣿⣾⣿⣷⣤⡿⠿⢿⣿⣿⣇⣿⡟⠋⠀⣿⡀⠀⣴⠲⡆⠀⠀⠸⣿⣿⣦⠀⠀⢸⡀⢹⣧⠀⠀⠀⣇⠀⠀⢹⠀⠀⠀⠸⣿⡟⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀  ⢽⡿⣷⠏⠛⠿⢠⣿⣿⣿⣿⢿⣯⡇⠀⠀⠈⠁⠀⠀⠀⠀⠀⢸⣇⠀⢻⠀⢳⠀⠀⠀⣿⣿⣿⣷⣾⢸⡇⠈⣿⡀⠀⠀⢸⠀⠀⠈⡇⠀⠀⢀⣿⣿⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "            ____        __  _                         ____       _              ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "           / __ \\____  / /_(_)___ ___  __  _______   / __ \\_____(_)___ ___  ___ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "          / / / / __ \\/ __/ / __ `__ \\/ / / / ___/  / /_/ / ___/ / __ `__ \\/ _ \\⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "         / /_/ / /_/ / /_/ / / / / / / /_/ (__  )  / ____/ /  / / / / / / /  __/⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "         \\____/ .___/\\__/_/_/ /_/ /_/\\__,_/____/  /_/   /_/  /_/_/ /_/ /_/\\___/ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "             /_/                                                                 ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "```\n",
    "\n",
    "## Overview\n",
    "This notebook trains the **OptimusPrime** transformer model to learn mappings from MD5 hash digests to plaintext passwords.\n",
    "\n",
    "- **Model**: Transformer encoder-decoder architecture\n",
    "- **Task**: Sequence-to-sequence learning (hash → password)\n",
    "- **Dataset**: 1M hash-password pairs\n",
    "- **Features**: TensorBoard logging, checkpoint saving/loading, evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sbdkvic8jos",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n",
    "Import all necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b54b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from model import OptimusPrime\n",
    "from data import Bumblebee, collate_batch, ALLOWED_PW_CHARS as PW_VOCAB, PAD_ID, SOS_ID, EOS_ID\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfbm9qg5iu",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set device, paths, training hyperparameters, and model architecture parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "xjlz4r9te6h",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Data: /home/laufeyson/xdev/school/dm/project/data/training/1M_train.tsv\n",
      "Model config: {'vocab_size': 257, 'pw_vocab_size': 77, 'pad_id': 74, 'sos_id': 75, 'eos_id': 76, 'd_model': 256, 'n_heads': 8, 'num_layers': 4, 'ff_dim': 512, 'dropout': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "path = Path.cwd().parent / 'data' / 'training' / '1M_train.tsv'\n",
    "\n",
    "# training hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 1024\n",
    "lr = 1e-4\n",
    "mode = 'latest' # 'best', 'latest', or 'none'\n",
    "\n",
    "# model architecture\n",
    "model_config = {\n",
    "    'vocab_size': 257,\n",
    "    'pw_vocab_size': len(PW_VOCAB),\n",
    "    'pad_id': PAD_ID,\n",
    "    'sos_id': SOS_ID,\n",
    "    'eos_id': EOS_ID,\n",
    "    'd_model': 256,\n",
    "    'n_heads': 8,\n",
    "    'num_layers': 4,\n",
    "    'ff_dim': 512,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "print(f'Device: {device}')\n",
    "print(f'Data: {path}')\n",
    "print(f'Model config: {model_config}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w34yj38jzlk",
   "metadata": {},
   "source": [
    "## 3. Dataset & DataLoader\n",
    "\n",
    "Load the hash-password dataset and create a batched DataLoader with custom collate function for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sbdtsu44zf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 996,035 samples\n",
      "Batches per epoch: 973\n"
     ]
    }
   ],
   "source": [
    "# create dataset and dataloader\n",
    "dataset = Bumblebee(path)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    collate_fn = collate_batch\n",
    ")\n",
    "\n",
    "# create evaluation dataloader \n",
    "# (optional: for automatic eval during training)\n",
    "eval_dataset = Bumblebee(Path.cwd().parent / 'data' / 'eval' / '1K_eval.tsv')\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, \n",
    "    batch_size = 128, \n",
    "    shuffle = False, \n",
    "    collate_fn = collate_batch, \n",
    "    drop_last = False\n",
    ")\n",
    "\n",
    "print(f'Dataset size: {len(dataset):,} samples')\n",
    "print(f'Batches per epoch: {len(dataloader):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iwoizk3l9g",
   "metadata": {},
   "source": [
    "## 4. Model Creation\n",
    "\n",
    "Initialize the OptimusPrime transformer encoder-decoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "p2a9iljec9p",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: OptimusPrime\n",
      "Total parameters: 5,902,669\n",
      "Trainable parameters: 5,902,669\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = OptimusPrime(**model_config)\n",
    "\n",
    "# count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Model: OptimusPrime')\n",
    "print(f'Total parameters: {total_params:,}')\n",
    "print(f'Trainable parameters: {trainable_params:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cgtrzfsmh",
   "metadata": {},
   "source": [
    "## 5. Optimizer\n",
    "\n",
    "Configure the Adam optimizer with the specified learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9881v8jhiqq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: Adam\n",
      "Learning rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# create optimizer\n",
    "optimizer = Adam(model.parameters(), lr = lr)\n",
    "\n",
    "print(f'Optimizer: Adam')\n",
    "print(f'Learning rate: {lr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psefc52rj4",
   "metadata": {},
   "source": [
    "## 6. Trainer Setup\n",
    "\n",
    "Create the Trainer instance with all components and initialize TensorBoard logging.\n",
    "\n",
    "### Loading Modes:\n",
    "- **`load_mode='latest'`**: Resume from most recent checkpoint (default)\n",
    "- **`load_mode='best'`**: Resume with best model weights, epoch from latest checkpoint\n",
    "- **`load_mode='none'`**: Start fresh training from epoch 0\n",
    "\n",
    "### Checkpoint Management:\n",
    "- **`max_checkpoints=5`**: Keep only 5 most recent checkpoint files (oldest auto-deleted)\n",
    "- **best_model.pt**: Always preserved, never auto-deleted\n",
    "\n",
    ">**Note**: Set `save = False` and `load_mode = 'none'` for testing without checkpoint I/O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9n1uqtuu54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m [SETUP]\u001b[0m\n",
      "  [total epochs]: 10\n",
      "  [device]: cuda\n",
      "  [moving model to device]\n",
      "  [initializing TensorBoard]\n",
      "  [creating checkpoint directory]\n",
      "\n",
      "\u001b[33m [LOADING LATEST CHECKPOINT]\u001b[0m\n",
      "  [load_mode]: latest\n",
      "  [loading]: checkpoint_epoch_19.pt\n",
      "  [resuming from epoch]: 19\n",
      "  [best loss so far]: 2.2375\n"
     ]
    }
   ],
   "source": [
    "# create trainer\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    optimizer = optimizer,\n",
    "    dataloader = dataloader,  # TRAINING dataloader\n",
    "    device = device,\n",
    "    epochs = epochs,\n",
    "    checkpoint_dir = Path('checkpoints'),\n",
    "    checkpoint_interval = 1,\n",
    "    logs = 'runs/optimus',\n",
    "    save = True,\n",
    "    load_mode = mode,  # 'latest', 'best', or 'none'\n",
    "    max_checkpoints = 5,  # keep only 5 most recent checkpoints\n",
    "    eval_dataloader = eval_dataloader  # EVAL dataloader; enables automatic eval every 10 epochs\n",
    ")\n",
    "\n",
    "# setup trainer\n",
    "trainer.setup()\n",
    "\n",
    "# load checkpoint if exists (skipped if load_mode = 'none')\n",
    "trainer.load(load_mode = mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uew222q9yi",
   "metadata": {},
   "source": [
    "## 7. Training\n",
    "\n",
    "Run the full training loop with gradient tracking, TensorBoard logging, and checkpoint saving.\n",
    "\n",
    "**Note**: If `eval_dataloader` was provided to the Trainer, evaluation will run automatically every 10 epochs during training, saving predictions to `predictions_ep_{N}.tsv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mkk6ugdczyn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m [START]\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/29: 100%|██████████| 973/973 [03:34<00:00,  4.54 batch/s, loss=2.2272, grad=0.452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [epoch 20 / 29 complete]\n",
      "  [loss]: 2.2272\n",
      "  [time]: 3.57 minutes\n",
      "    [checkpoint saved]: checkpoint_epoch_20.pt\n",
      "    [cleanup]: removed checkpoint_epoch_1.pt\n",
      "    [cleanup]: removed checkpoint_epoch_2.pt\n",
      "    [cleanup]: removed checkpoint_epoch_3.pt\n",
      "    [cleanup]: removed checkpoint_epoch_4.pt\n",
      "    [cleanup]: removed checkpoint_epoch_5.pt\n",
      "    [cleanup]: removed checkpoint_epoch_6.pt\n",
      "    [cleanup]: removed checkpoint_epoch_7.pt\n",
      "    [cleanup]: removed checkpoint_epoch_8.pt\n",
      "    [cleanup]: removed checkpoint_epoch_9.pt\n",
      "    [cleanup]: removed checkpoint_epoch_10.pt\n",
      "    [cleanup]: removed checkpoint_epoch_11.pt\n",
      "    [cleanup]: removed checkpoint_epoch_12.pt\n",
      "    [cleanup]: removed checkpoint_epoch_13.pt\n",
      "    [cleanup]: removed checkpoint_epoch_14.pt\n",
      "    [cleanup]: removed checkpoint_epoch_15.pt\n",
      "\u001b[32m    [new best saved]: 2.2375 -> 2.2272\u001b[0m\n",
      "\n",
      "\u001b[36m [STARTING EVALUATION at epoch 20]\u001b[0m\n",
      "\n",
      "\u001b[34m [EVALUATION]\u001b[0m\n",
      "  [saving predictions for epoch 20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 8/8 [00:00<00:00, 14.48 batch/s, loss = 2.4893, exact = 0.0000, char = 0.2832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [evaluation complete]\n",
      "  [loss]: 2.1781\n",
      "  [exact match]: 0.0000 (0/1000)\n",
      "  [char similarity]: 0.2832\n",
      "  [levenshtein]: 0.3007\n",
      "  [jaccard]: 0.3498\n",
      "  [predictions saved]: predictions_ep_20.tsv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/29: 100%|██████████| 973/973 [03:34<00:00,  4.53 batch/s, loss=2.2226, grad=0.436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [epoch 21 / 29 complete]\n",
      "  [loss]: 2.2226\n",
      "  [time]: 3.58 minutes\n",
      "    [checkpoint saved]: checkpoint_epoch_21.pt\n",
      "    [cleanup]: removed checkpoint_epoch_16.pt\n",
      "\u001b[32m    [new best saved]: 2.2272 -> 2.2226\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/29: 100%|██████████| 973/973 [03:34<00:00,  4.54 batch/s, loss=2.2178, grad=0.465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [epoch 22 / 29 complete]\n",
      "  [loss]: 2.2178\n",
      "  [time]: 3.57 minutes\n",
      "    [checkpoint saved]: checkpoint_epoch_22.pt\n",
      "    [cleanup]: removed checkpoint_epoch_17.pt\n",
      "\u001b[32m    [new best saved]: 2.2226 -> 2.2178\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/29: 100%|██████████| 973/973 [03:34<00:00,  4.54 batch/s, loss=2.2136, grad=0.472]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [epoch 23 / 29 complete]\n",
      "  [loss]: 2.2136\n",
      "  [time]: 3.57 minutes\n",
      "    [checkpoint saved]: checkpoint_epoch_23.pt\n",
      "    [cleanup]: removed checkpoint_epoch_18.pt\n",
      "\u001b[32m    [new best saved]: 2.2178 -> 2.2136\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/29: 100%|██████████| 973/973 [03:34<00:00,  4.53 batch/s, loss=2.2096, grad=0.454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [epoch 24 / 29 complete]\n",
      "  [loss]: 2.2096\n",
      "  [time]: 3.58 minutes\n",
      "    [checkpoint saved]: checkpoint_epoch_24.pt\n",
      "    [cleanup]: removed checkpoint_epoch_19.pt\n",
      "\u001b[32m    [new best saved]: 2.2136 -> 2.2096\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/29: 100%|██████████| 973/973 [03:34<00:00,  4.54 batch/s, loss=2.2055, grad=0.458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [epoch 25 / 29 complete]\n",
      "  [loss]: 2.2055\n",
      "  [time]: 3.58 minutes\n",
      "    [checkpoint saved]: checkpoint_epoch_25.pt\n",
      "    [cleanup]: removed checkpoint_epoch_20.pt\n",
      "\u001b[32m    [new best saved]: 2.2096 -> 2.2055\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/29: 100%|██████████| 973/973 [03:34<00:00,  4.53 batch/s, loss=2.2017, grad=0.501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [epoch 26 / 29 complete]\n",
      "  [loss]: 2.2017\n",
      "  [time]: 3.58 minutes\n",
      "    [checkpoint saved]: checkpoint_epoch_26.pt\n",
      "    [cleanup]: removed checkpoint_epoch_21.pt\n",
      "\u001b[32m    [new best saved]: 2.2055 -> 2.2017\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/29: 100%|██████████| 973/973 [03:34<00:00,  4.54 batch/s, loss=2.1979, grad=0.468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [epoch 27 / 29 complete]\n",
      "  [loss]: 2.1979\n",
      "  [time]: 3.57 minutes\n",
      "    [checkpoint saved]: checkpoint_epoch_27.pt\n",
      "    [cleanup]: removed checkpoint_epoch_22.pt\n",
      "\u001b[32m    [new best saved]: 2.2017 -> 2.1979\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/29: 100%|██████████| 973/973 [03:34<00:00,  4.54 batch/s, loss=2.1946, grad=0.471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [epoch 28 / 29 complete]\n",
      "  [loss]: 2.1946\n",
      "  [time]: 3.57 minutes\n",
      "    [checkpoint saved]: checkpoint_epoch_28.pt\n",
      "    [cleanup]: removed checkpoint_epoch_23.pt\n",
      "\u001b[32m    [new best saved]: 2.1979 -> 2.1946\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/29: 100%|██████████| 973/973 [03:34<00:00,  4.54 batch/s, loss=2.1914, grad=0.510]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [epoch 29 / 29 complete]\n",
      "  [loss]: 2.1914\n",
      "  [time]: 3.57 minutes\n",
      "    [checkpoint saved]: checkpoint_epoch_29.pt\n",
      "    [cleanup]: removed checkpoint_epoch_24.pt\n",
      "\u001b[32m    [new best saved]: 2.1946 -> 2.1914\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8vs9tp4nv8e",
   "metadata": {},
   "source": [
    "## 8. Evaluation\n",
    "\n",
    "Evaluate the trained model on a held-out test set with multiple similarity metrics:\n",
    "- **Exact Match**: Perfect character-by-character match\n",
    "- **Char Similarity**: Positional character matching\n",
    "- **Levenshtein**: Normalized edit distance\n",
    "- **Jaccard**: Character set overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ad021",
   "metadata": {},
   "source": [
    "Results are __NOT__ logged to TensorBoard\n",
    "\n",
    ">NOTE: Use when testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18168eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate WITHOUT step parameter (standalone mode):\n",
    "#   - No TensorBoard logging\n",
    "#   - No prediction saving\n",
    "# results = trainer.eval(eval_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef780d85",
   "metadata": {},
   "source": [
    "Results __are__ logged to TensorBoard under the `Eval/` namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lso8lej4t",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "# Note: eval already ran automatically every 10 epochs during training if eval_dataloader was provided to Trainer\n",
    "\n",
    "# create evaluation dataloader \n",
    "# (if not already created in section 6)\n",
    "\n",
    "# -- UNCOMMENT FOR STANDALONE EVALUATIONS --\n",
    "# if 'eval_dataloader' not in dir():\n",
    "#     eval_dataset = Bumblebee(Path.cwd().parent / 'data' / 'eval' / '1K_eval.tsv')\n",
    "#     eval_dataloader = DataLoader(eval_dataset, batch_size = 128, shuffle = False, collate_fn = collate_batch, drop_last = False)\n",
    "\n",
    "# # evaluate WITH step parameter (tracked mode):\n",
    "# #   - logs metrics to TensorBoard under 'Eval/' namespace\n",
    "# #   - saves predictions to predictions_ep_{step}.tsv if step % 10 == 0\n",
    "# results = trainer.eval(eval_dataloader, step = trainer.start_epoch + trainer.epochs)\n",
    "\n",
    "# # print results:\n",
    "# print(f\"\\nEval Results:\")\n",
    "# print(f\"  Loss: {results['loss']:.4f}\")\n",
    "# print(f\"  Exact Match: {results['exact_match']:.4f}\")\n",
    "# print(f\"  Char Similarity: {results['char_similarity']:.4f}\")\n",
    "# print(f\"  Levenshtein: {results['levenshtein']:.4f}\")\n",
    "# print(f\"  Jaccard: {results['jaccard']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j8gvuhyikq",
   "metadata": {},
   "source": [
    "## 9. Cleanup\n",
    "\n",
    "Close TensorBoard writer and display training summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pksq233fyiq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m [COMPLETE]\u001b[0m\n",
      "  [best loss]: 2.1914\n",
      "  [checkpoints saved]: checkpoints\n",
      "\n",
      " [TensorBoard]: \u001b[33mtensorboard --logdir=runs\u001b[0m\n",
      "  [TensorBoard dashboard]: http://localhost:6006\n",
      "  [upload TensorBoard logs]:\n",
      "\t\u001b[33mtensorboard dev upload --logdir runs/optimus_prime\u001b[0m \\            \n",
      "\t\t\u001b[33m--name \u001b[0m\u001b[36m\"Optimus Prime - MD5 Hash Inversion\"\u001b[0m \\            \n",
      "\t\t\u001b[33m--description \u001b[0m\u001b[36m\"Training run with 1M password dataset\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# cleanup and display summary\n",
    "trainer.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
